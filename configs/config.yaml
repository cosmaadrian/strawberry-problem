env: genesis

seed: 666
run_id: 1

accumulation_steps: 1
clip_grad_norm: 1
max_grad_norm: 1.5

resume_from: ""

evaluators:
  - name: generation
    args:
      solution: 42

epochs: -1
n_train_iters: 200000
actually_stop_at: 25000

batch_size: 64
eval_every: 1 # epochs
eval_batch_size: 32
eval_every_batches: 4096

trainer: lm_trainer

dataset: pretraining
dataset_args:
  dataset_name: wikimedia/wikipedia
  subset: 20231101.en
  chunk_size: 256

model_checkpoint:
  save_model: 1
  override_checkpoints: 1
  monitor_quantity: "generation#loss_next_byte"
  direction: down
  base_dir: 'checkpoints/'

model: llm
model_args:
  input_tokenizer: openai-community/gpt2 # used for the byte tokenizer
  dmodel: 512
  num_layers: 8
  dropout: 0.0
  use_char_context: 0
  char_llm_args:
    dmodel: 256
    num_layers: 1
    dropout: 0.0
    context_position: [0, 1, 2, 3, 4, 5, 6, 7]

####################################################
####################################################
####################################################

model_width_multiplier: 1.0

####################################################
####################################################
####################################################

optimizer_args:
  beta1: 0.9
  beta2: 0.95
  eps: 1e-10
  weight_decay: 0.01

scheduler_args:
  max_lr: 0.00001
  base_batch_size: 128
  num_warmup_steps: 256
  reset_scheduler: 0

log_grads: 0
log_every: 128 # batches

use_compile: 0 
use_amp: 1